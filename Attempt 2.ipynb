{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "diverse-morocco",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import numpy as np\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cardiovascular-hurricane",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_data = np.load(\"./Assignment1-Dataset/train_data.npy\")\n",
    "train_label = np.load(\"./Assignment1-Dataset/train_label.npy\")\n",
    "test_data = np.load(\"./Assignment1-Dataset/test_data.npy\")\n",
    "test_label = np.load(\"./Assignment1-Dataset/test_label.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "patient-michael",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 128)\n",
      "(50000, 1)\n",
      "(10000, 128)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks\n",
    "print(train_data.shape)\n",
    "print(train_label.shape)\n",
    "print(test_data.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "brown-organization",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gaussian_weights(num_neurons, num_features):\n",
    "    '''\n",
    "    Generate weights taken from the Gaussian distribution, and based on the number of neurons within the hidden layer\n",
    "    as well as the number of features of the dataset\n",
    "    \n",
    "    Output is weights matrix of size (num_neurons, num_features)\n",
    "    '''\n",
    "    \n",
    "    weights = norm.rvs(size = [num_neurons, num_features], random_state=1)\n",
    "    \n",
    "    return weights\n",
    "\n",
    "def generate_gaussian_bias(num_neurons):\n",
    "    '''\n",
    "    Generate bias vector from the Gaussian distribution, and based on the number of neurons within the hidden layer\n",
    "    \n",
    "    Output is bias vector of size (num_neurons)\n",
    "    '''\n",
    "    \n",
    "    bias = norm.rvs(size = num_neurons, random_state=1)\n",
    "    \n",
    "    return bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "large-tactics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_z(data_vector, weights_matrix, bias_vector):\n",
    "    '''\n",
    "    Calculate the z value for all the neurons within the specific hidden layer, obtained by taking the dot product\n",
    "    between the weights matrix and data vector.  The bias vector is then added onto the product of the two.\n",
    "    \n",
    "    The output vector then represents the input value to be used for the activation function of all the neurons within\n",
    "    the specific hidden layer.\n",
    "    '''\n",
    "    \n",
    "    return weights_matrix.dot(data_vector) + bias_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "strong-information",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_activation_func(activation_func, z):\n",
    "    '''\n",
    "    Calculates the value after the z has been computed and puts it inside the non-linear activation function\n",
    "    that we have for that hidden layer\n",
    "    '''\n",
    "    \n",
    "    if activation_func == 'relu':\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    if activation_func == 'softmax':\n",
    "        return np.divide(np.exp(z), np.sum(np.exp(z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "addressed-payroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_label_vector(label_vector):\n",
    "    '''\n",
    "    Encode the label vector of our dataset, such that we can use it for the computation of the MSE.\n",
    "    This is because the labels are labelled as integers 0 to 9, whereas for MSE to work, we need to\n",
    "    create a array vector of size 10 for every single observation, where the value 1 is set on the index of the correct observation\n",
    "    '''\n",
    "    \n",
    "    num_classes = np.unique(train_label).size\n",
    "    \n",
    "    encoded_label_vector = []\n",
    "    \n",
    "    for label in label_vector:\n",
    "        encoded_label = np.zeros(num_classes)\n",
    "        encoded_label[label] = 1\n",
    "        encoded_label_vector.append(encoded_label)\n",
    "    \n",
    "    encoded_label_vector = np.array(encoded_label_vector)\n",
    "    \n",
    "    return encoded_label_vector\n",
    "    \n",
    "\n",
    "def calculate_MSE(pred, actual):\n",
    "    '''\n",
    "    Calculates the Mean Squared Error between the prediction of the NN and actual class\n",
    "    \n",
    "    Note:\n",
    "    This works because the pred value is the softmax of the output of the NN and\n",
    "    the actual is adjusted such that the values are between 0 and 1\n",
    "    '''\n",
    "    error = np.subtract(pred, actual)\n",
    "    squared_error = np.square(error)\n",
    "    return np.sum(squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "registered-utility",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_delta_softmax(layer_output, encoded_label_vector):\n",
    "    '''\n",
    "    Calculates the delta value for the final/output layer, which will be used for backpropagation\n",
    "    \n",
    "    Note:\n",
    "    This function expects to receive the output of the softmax activation function within the output layer\n",
    "    and also the encoded label vector which would have values 0 and 1 exclusively\n",
    "    \n",
    "    delta = activation_output - y\n",
    "    \n",
    "    Output: vector of size num of classes to be predicted\n",
    "    '''\n",
    "    \n",
    "    return np.subtract(layer_output, encoded_label_vector)\n",
    "\n",
    "\n",
    "def calc_delta_hidden(l_plus_one_delta, l_plus_one_weights, layer_output):\n",
    "    '''\n",
    "    Calculates the delta value for the hidden layers, which will be used for backpropagation\n",
    "    \n",
    "    This function is only to be used to calculate the delta values of the hidden layers.  Output layer delta should be\n",
    "    the calc_delta_softmax, and there is no delta term needed to be calculated for input layer (NN layer 1)\n",
    "    \n",
    "    delta = (weights for next layer . delta for next layer) .* (activation_func output for this layer .* (1 - activation_func output for this layer))\n",
    "    '''\n",
    "    \n",
    "    first_part = (l_plus_one_weights.T).dot(l_plus_one_delta)\n",
    "    second_part = np.multiply(layer_output, np.ones(layer_output.size) - layer_output)\n",
    "    return  np.multiply(first_part, second_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "printable-husband",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight and bias generated for hidden layer 1 with weight shape (5, 128) and bias shape of (5,)\n",
      "Weight and bias generated for hidden layer 2 with weight shape (3, 5) and bias shape of (3,)\n",
      "Weight and bias generated for hidden layer 3 with weight shape (6, 3) and bias shape of (6,)\n",
      "1.9998233671553345\n",
      "1.6193690828534564\n",
      "2.0\n",
      "[1.8730641500029304]\n",
      "Shape before conversion: observations are 3, hidden + output layers are 4, first layer neurons are (5,)\n",
      "Shape after conversion (3, 4), num of neurons in the first layer (5,)\n",
      "Shape of our averaged delta (4,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-54-4372c0eb4365>:109: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  weight_matrix = np.subtract(weight_matrix, (LEARNING_RATE * avg_delta))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (5,128) (5,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-4372c0eb4365>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;31m# Time to update our weights, using the averaged delta from previous calc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m \u001b[0mweight_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mavg_delta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (5,128) (5,) "
     ]
    }
   ],
   "source": [
    "# Let's do a test-run of the functions above in setting up feedforward\n",
    "\n",
    "\n",
    "### Initialisation\n",
    "# Make sure to encode the label vector\n",
    "encoded_label_vector = encode_label_vector(train_label)\n",
    "\n",
    "hidden_layers = 3 # this should be int\n",
    "hidden_layers_activation_func = ['relu', 'relu', 'relu']\n",
    "num_neurons = [5,3,6] # this should be a list containing int per hidden layer\n",
    "num_classes = np.unique(train_label).size\n",
    "\n",
    "weight_matrix = []\n",
    "bias_vector = []\n",
    "\n",
    "# initialise the weights\n",
    "for layer_num, layer in enumerate(range(hidden_layers)):\n",
    "    \n",
    "    # If we are instantiating the details for the first hidden layer, then make the following adjustments\n",
    "    # which would otherwise be not required for subsequent hidden layers\n",
    "    if layer_num == 0:\n",
    "        # The input features would be the shape of our dataset instead of num of features from previous layer\n",
    "        num_input_features = train_data.shape[1]\n",
    "    else:\n",
    "        num_input_features = num_neurons[layer_num - 1]\n",
    "    \n",
    "    # check how many neurons should be in this layer\n",
    "    neuron_num = num_neurons[layer_num]\n",
    "    \n",
    "    layer_weights = generate_gaussian_weights(neuron_num, num_input_features)\n",
    "    layer_bias = generate_gaussian_bias(neuron_num)\n",
    "    \n",
    "    weight_matrix.append(layer_weights)\n",
    "    bias_vector.append(layer_bias)\n",
    "    \n",
    "    print(f'Weight and bias generated for hidden layer {layer_num + 1} with weight shape {weight_matrix[layer_num].shape} and bias shape of {bias_vector[layer_num].shape}')\n",
    "\n",
    "   \n",
    "# insantiate the parts for the output layer\n",
    "# need to be very careful with the use of -1 indices, in the event that we incorporate output layer to our hidden layer variables\n",
    "weight_matrix.append(generate_gaussian_weights(num_classes, num_neurons[-1]))\n",
    "bias_vector.append(generate_gaussian_bias(num_classes))\n",
    "\n",
    "\n",
    "### Getting the network running\n",
    "BATCH_SIZE = 3\n",
    "LEARNING_RATE = 0.01\n",
    "batch_loss = []\n",
    "epoch_loss = []\n",
    "\n",
    "# feedforward part\n",
    "# for the two variables below, the expected final state is numpy_array(representing each layer)\n",
    "# in a list (representing each observation)\n",
    "# in a list (the final container of the object)\n",
    "layer_z = [[] for i in range(BATCH_SIZE)]\n",
    "layer_output = [[] for i in range(BATCH_SIZE)]\n",
    "\n",
    "for observation_idx, observation_val in enumerate(range(BATCH_SIZE)):\n",
    "    \n",
    "    for layer_num, layer in enumerate(range(hidden_layers)):\n",
    "    \n",
    "        if layer_num == 0:\n",
    "            input_data = train_data[observation_idx]\n",
    "        else:\n",
    "            # extract the output of the previous layer\n",
    "            input_data = layer_output[observation_idx][layer_num - 1]\n",
    "        \n",
    "        z = calc_z(input_data, weight_matrix[layer_num], bias_vector[layer_num])\n",
    "        a = run_activation_func(hidden_layers_activation_func[layer_num], z)\n",
    "    \n",
    "        layer_z[observation_idx].append(z)\n",
    "        layer_output[observation_idx].append(a)\n",
    "    \n",
    "    # Calculation for the output layer\n",
    "    z = calc_z(layer_output[observation_idx][-1], weight_matrix[-1], bias_vector[-1])\n",
    "    a = run_activation_func('softmax', z)\n",
    "    layer_z[observation_idx].append(z) # to be used for backpropagation\n",
    "    layer_output[observation_idx].append(a)\n",
    "\n",
    "    # Calculate the error\n",
    "    loss = calculate_MSE(layer_output[observation_idx][-1], encoded_label_vector[observation_idx]) \n",
    "    batch_loss.append(loss)\n",
    "    print(loss)\n",
    "\n",
    "epoch_loss.append(np.average(batch_loss))\n",
    "print(epoch_loss)\n",
    "\n",
    "# Perform the backpropagation\n",
    "# instantiate the delta list obj for hidden_layers + output layer\n",
    "delta = [[0 for i in range(hidden_layers + 1)] for i in range(BATCH_SIZE)]\n",
    "d = [[0 for i in range(hidden_layers + 1)] for i in range(BATCH_SIZE)]\n",
    "\n",
    "for observation_idx, observation_val in enumerate(range(BATCH_SIZE)):\n",
    "    # for the output layer\n",
    "    delta[observation_idx][hidden_layers] = (calc_delta_softmax(layer_output[observation_idx][-1], encoded_label_vector[observation_idx]))\n",
    "\n",
    "    for layer_num, layer in reversed(list(enumerate(range(hidden_layers)))):\n",
    "        delta[observation_idx][layer_num] = calc_delta_hidden(delta[observation_idx][layer_num + 1], weight_matrix[layer_num + 1], layer_output[observation_idx][layer_num])\n",
    "        \n",
    "        \n",
    "# Time to calculate the average delta from across different observations\n",
    "print(f'Shape before conversion: observations are {len(delta)}, hidden + output layers are {len(delta[0])}, first layer neurons are {delta[0][0].shape}')\n",
    "delta = np.array(delta, dtype=object) # quite important to convert this list into a pure numpy array for avg next\n",
    "print(f'Shape after conversion {delta.shape}, num of neurons in the first layer {delta[0][0].shape}')\n",
    "avg_delta = np.average(delta, axis = 0) # each array within the array, then represents the neurons within a layer\n",
    "print(f'Shape of our averaged delta {avg_delta.shape}')\n",
    "\n",
    "\n",
    "# Time to update our weights, using the averaged delta from previous calc\n",
    "weight_matrix = np.subtract(weight_matrix, (LEARNING_RATE * avg_delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "retained-product",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_dummy = np.array(weight_matrix, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aware-briefing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_delta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "billion-rotation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 128)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_dummy[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "threatened-school",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-loading",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
