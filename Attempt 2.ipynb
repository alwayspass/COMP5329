{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "diverse-morocco",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import numpy as np\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cardiovascular-hurricane",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_df = np.load(\"./Assignment1-Dataset/train_data.npy\")\n",
    "train_label = np.load(\"./Assignment1-Dataset/train_label.npy\")\n",
    "test_df = np.load(\"./Assignment1-Dataset/test_data.npy\")\n",
    "test_label = np.load(\"./Assignment1-Dataset/test_label.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "patient-michael",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 128)\n",
      "(50000, 1)\n",
      "(10000, 128)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks\n",
    "print(train_df.shape)\n",
    "print(train_label.shape)\n",
    "print(test_df.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "brown-organization",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gaussian_weights(num_neurons, num_features):\n",
    "    '''\n",
    "    Generate weights taken from the Gaussian distribution, and based on the number of neurons within the hidden layer\n",
    "    as well as the number of features of the dataset\n",
    "    \n",
    "    Output is weights matrix of size (num_neurons, num_features)\n",
    "    '''\n",
    "    \n",
    "    weights = norm.rvs(size = [num_neurons, num_features])\n",
    "    \n",
    "    return weights\n",
    "\n",
    "def generate_gaussian_bias(num_neurons):\n",
    "    '''\n",
    "    Generate bias vector from the Gaussian distribution, and based on the number of neurons within the hidden layer\n",
    "    \n",
    "    Output is bias vector of size (num_neurons)\n",
    "    '''\n",
    "    \n",
    "    bias = norm.rvs(size = num_neurons)\n",
    "    \n",
    "    return bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "large-tactics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_z(data_vector, weights_matrix, bias_vector):\n",
    "    '''\n",
    "    Calculate the z value for all the neurons within the specific hidden layer, obtained by taking the dot product\n",
    "    between the weights matrix and data vector.  The bias vector is then added onto the product of the two.\n",
    "    \n",
    "    The output vector then represents the input value to be used for the activation function of all the neurons within\n",
    "    the specific hidden layer.\n",
    "    '''\n",
    "    \n",
    "    return weights_matrix.dot(data_vector) + bias_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "strong-information",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_activation_func(activation_func, z):\n",
    "    '''\n",
    "    Calculates the value after the z has been computed and puts it inside the non-linear activation function\n",
    "    that we have for that hidden layer\n",
    "    '''\n",
    "    \n",
    "    if activation_func == 'relu':\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    if activation_func == 'softmax':\n",
    "        return np.divide(np.exp(z), np.sum(np.exp(z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "addressed-payroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_label_vector(label_vector):\n",
    "    '''\n",
    "    Encode the label vector of our dataset, such that we can use it for the computation of the MSE.\n",
    "    This is because the labels are labelled as integers 0 to 9, whereas for MSE to work, we need to\n",
    "    create a array vector of size 10 for every single observation, where the value 1 is set on the index of the correct observation\n",
    "    '''\n",
    "    \n",
    "    num_classes = np.unique(train_label).size\n",
    "    \n",
    "    encoded_label_vector = []\n",
    "    \n",
    "    for label in label_vector:\n",
    "        encoded_label = np.zeros(num_classes)\n",
    "        encoded_label[label] = 1\n",
    "        encoded_label_vector.append(encoded_label)\n",
    "    \n",
    "    encoded_label_vector = np.array(encoded_label_vector)\n",
    "    \n",
    "    return encoded_label_vector\n",
    "    \n",
    "\n",
    "def calculate_MSE(pred, actual):\n",
    "    '''\n",
    "    Calculates the Mean Squared Error between the prediction of the NN and actual class\n",
    "    \n",
    "    Note:\n",
    "    This works because the pred value is the softmax of the output of the NN and\n",
    "    the actual is adjusted such that the values are between 0 and 1\n",
    "    '''\n",
    "    error = np.subtract(pred, actual)\n",
    "    squared_error = np.square(error)\n",
    "    return np.sum(squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "printable-husband",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight and bias generated for hidden layer 1 with weight shape (5, 128) and bias shape of (5,)\n",
      "Weight and bias generated for hidden layer 2 with weight shape (3, 5) and bias shape of (3,)\n",
      "Weight and bias generated for hidden layer 3 with weight shape (6, 3) and bias shape of (6,)\n",
      "1.503497156339467\n"
     ]
    }
   ],
   "source": [
    "# Let's do a test-run of the functions above in setting up feedforward\n",
    "\n",
    "# Make sure to encode the label vector\n",
    "encoded_label_vector = encode_label_vector(train_label)\n",
    "\n",
    "hidden_layers = 3 # this should be int\n",
    "num_neurons = [5,3,6] # this should be a list containing int per hidden layer\n",
    "num_classes = np.unique(train_label).size\n",
    "\n",
    "weight_matrix = []\n",
    "bias_vector = []\n",
    "\n",
    "# initialise the weights\n",
    "for layer_num, layer in enumerate(range(hidden_layers)):\n",
    "    \n",
    "    # If we are instantiating the details for the first hidden layer, then make the following adjustments\n",
    "    # which would otherwise be not required for subsequent hidden layers\n",
    "    if layer_num == 0:\n",
    "        # The input features would be the shape of our dataset instead of num of features from previous layer\n",
    "        num_input_features = train_df.shape[1]\n",
    "    else:\n",
    "        num_input_features = num_neurons[layer_num - 1]\n",
    "    \n",
    "    # check how many neurons should be in this layer\n",
    "    neuron_num = num_neurons[layer_num]\n",
    "    \n",
    "    layer_weights = generate_gaussian_weights(neuron_num, num_input_features)\n",
    "    layer_bias = generate_gaussian_bias(neuron_num)\n",
    "    \n",
    "    weight_matrix.append(layer_weights)\n",
    "    bias_vector.append(layer_bias)\n",
    "    \n",
    "    print(f'Weight and bias generated for hidden layer {layer_num + 1} with weight shape {weight_matrix[layer_num].shape} and bias shape of {bias_vector[layer_num].shape}')\n",
    "\n",
    "   \n",
    "\n",
    "# insantiate the parts for the output layer\n",
    "# need to be very careful with the use of -1 indices, in the event that we incorporate output layer to our hidden layer variables\n",
    "weight_matrix.append(generate_gaussian_weights(num_classes, num_neurons[-1]))\n",
    "bias_vector.append(generate_gaussian_bias(num_classes))\n",
    "\n",
    "\n",
    "# feedforward part\n",
    "layer_output = []\n",
    "for layer_num, layer in enumerate(range(hidden_layers)):\n",
    "    \n",
    "    if layer_num == 0:\n",
    "        input_data = train_df[0] # hard coding this for now, will set batches later\n",
    "    else:\n",
    "        # extract the output of the previous layer\n",
    "        input_data = layer_output[layer_num - 1]\n",
    "        \n",
    "    z = calc_z(input_data, weight_matrix[layer_num], bias_vector[layer_num])\n",
    "    a = run_activation_func('relu', z)\n",
    "    \n",
    "    layer_output.append(a)\n",
    "    \n",
    "    \n",
    "# Calculation for the output layer\n",
    "z = calc_z(layer_output[-1], weight_matrix[-1], bias_vector[-1])\n",
    "a = run_activation_func('softmax', z)\n",
    "layer_output.append(a)\n",
    "\n",
    "# Calculate the error\n",
    "print(calculate_MSE(layer_output[-1], encoded_label_vector[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aware-authority",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This should equal the number of hidden layers: 3\n",
      "Example output for the first layer (5,)\n"
     ]
    }
   ],
   "source": [
    "print(f'This should equal the number of hidden layers: {len(layer_output)}')\n",
    "print(f'Example output for the first layer {layer_output[0].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aquatic-syntax",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(layer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "tutorial-profile",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.59576680e-013, 2.35207458e-059, 4.15921888e-112, 1.31809348e-029,\n",
       "       2.13782186e-014, 7.34339355e-095, 6.64429374e-054, 2.21452093e-069,\n",
       "       1.43755567e-086, 1.00000000e+000])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_output[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "verbal-accused",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "retired-tiffany",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6], dtype=uint8)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "spectacular-walnut",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_label_vector[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cutting-colombia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6],\n",
       "       [9],\n",
       "       [9],\n",
       "       [4],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [7],\n",
       "       [8],\n",
       "       [3]], dtype=uint8)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confused-composition",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
