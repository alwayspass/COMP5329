{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "diverse-morocco",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import numpy as np\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cardiovascular-hurricane",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_data = np.load(\"./Assignment1-Dataset/train_data.npy\")\n",
    "train_label = np.load(\"./Assignment1-Dataset/train_label.npy\")\n",
    "test_data = np.load(\"./Assignment1-Dataset/test_data.npy\")\n",
    "test_label = np.load(\"./Assignment1-Dataset/test_label.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "patient-michael",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 128)\n",
      "(50000, 1)\n",
      "(10000, 128)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks\n",
    "print(train_data.shape)\n",
    "print(train_label.shape)\n",
    "print(test_data.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "brown-organization",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gaussian_weights(num_neurons, num_features):\n",
    "    '''\n",
    "    Generate weights taken from the Gaussian distribution, and based on the number of neurons within the hidden layer\n",
    "    as well as the number of features of the dataset\n",
    "    \n",
    "    Output is weights matrix of size (num_neurons, num_features)\n",
    "    '''\n",
    "    \n",
    "    weights = norm.rvs(size = [num_neurons, num_features], random_state=1)\n",
    "    \n",
    "    return weights\n",
    "\n",
    "def generate_gaussian_bias(num_neurons):\n",
    "    '''\n",
    "    Generate bias vector from the Gaussian distribution, and based on the number of neurons within the hidden layer\n",
    "    \n",
    "    Output is bias vector of size (num_neurons)\n",
    "    '''\n",
    "    \n",
    "    bias = norm.rvs(size = num_neurons, random_state=1)\n",
    "    \n",
    "    return bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "large-tactics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_z(data_vector, weights_matrix, bias_vector):\n",
    "    '''\n",
    "    Calculate the z value for all the neurons within the specific hidden layer, obtained by taking the dot product\n",
    "    between the weights matrix and data vector.  The bias vector is then added onto the product of the two.\n",
    "    \n",
    "    The output vector then represents the input value to be used for the activation function of all the neurons within\n",
    "    the specific hidden layer.\n",
    "    '''\n",
    "    \n",
    "    return weights_matrix.dot(data_vector) + bias_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "strong-information",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_activation_func(activation_func, z):\n",
    "    '''\n",
    "    Calculates the value after the z has been computed and puts it inside the non-linear activation function\n",
    "    that we have for that hidden layer\n",
    "    '''\n",
    "    \n",
    "    if activation_func == 'relu':\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    if activation_func == 'softmax':\n",
    "        return np.divide(np.exp(z), np.sum(np.exp(z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "addressed-payroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_label_vector(label_vector):\n",
    "    '''\n",
    "    Encode the label vector of our dataset, such that we can use it for the computation of the MSE.\n",
    "    This is because the labels are labelled as integers 0 to 9, whereas for MSE to work, we need to\n",
    "    create a array vector of size 10 for every single observation, where the value 1 is set on the index of the correct observation\n",
    "    '''\n",
    "    \n",
    "    num_classes = np.unique(train_label).size\n",
    "    \n",
    "    encoded_label_vector = []\n",
    "    \n",
    "    for label in label_vector:\n",
    "        encoded_label = np.zeros(num_classes)\n",
    "        encoded_label[label] = 1\n",
    "        encoded_label_vector.append(encoded_label)\n",
    "    \n",
    "    encoded_label_vector = np.array(encoded_label_vector)\n",
    "    \n",
    "    return encoded_label_vector\n",
    "    \n",
    "\n",
    "def calculate_MSE(pred, actual):\n",
    "    '''\n",
    "    Calculates the Mean Squared Error between the prediction of the NN and actual class\n",
    "    \n",
    "    Note:\n",
    "    This works because the pred value is the softmax of the output of the NN and\n",
    "    the actual is adjusted such that the values are between 0 and 1\n",
    "    '''\n",
    "    error = np.subtract(pred, actual)\n",
    "    squared_error = np.square(error)\n",
    "    return np.sum(squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "registered-utility",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_delta_softmax(layer_output, encoded_label_vector):\n",
    "    '''\n",
    "    Calculates the delta value for the final/output layer, which will be used for backpropagation\n",
    "    \n",
    "    Note:\n",
    "    This function expects to receive the output of the softmax activation function within the output layer\n",
    "    and also the encoded label vector which would have values 0 and 1 exclusively\n",
    "    \n",
    "    delta = activation_output - y\n",
    "    \n",
    "    Output: vector of size num of classes to be predicted\n",
    "    '''\n",
    "    \n",
    "    return np.subtract(layer_output, encoded_label_vector)\n",
    "\n",
    "\n",
    "def calc_delta_hidden(l_plus_one_delta, l_plus_one_weights, layer_output):\n",
    "    '''\n",
    "    Calculates the delta value for the hidden layers, which will be used for backpropagation\n",
    "    \n",
    "    This function is only to be used to calculate the delta values of the hidden layers.  Output layer delta should be\n",
    "    the calc_delta_softmax, and there is no delta term needed to be calculated for input layer (NN layer 1)\n",
    "    \n",
    "    delta = (weights for next layer . delta for next layer) .* (activation_func output for this layer .* (1 - activation_func output for this layer))\n",
    "    '''\n",
    "    \n",
    "    first_part = (l_plus_one_weights.T).dot(l_plus_one_delta)\n",
    "    second_part = np.multiply(layer_output, np.ones(layer_output.size) - layer_output)\n",
    "    return  np.multiply(first_part, second_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "printable-husband",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight and bias generated for hidden layer 1 with weight shape (5, 128) and bias shape of (5,)\n",
      "Weight and bias generated for hidden layer 2 with weight shape (3, 5) and bias shape of (3,)\n",
      "Weight and bias generated for hidden layer 3 with weight shape (6, 3) and bias shape of (6,)\n",
      "1.9998233671553345\n",
      "Layer num is 2\n",
      "Layer num is 1\n",
      "Layer num is 0\n"
     ]
    }
   ],
   "source": [
    "# Let's do a test-run of the functions above in setting up feedforward\n",
    "\n",
    "# Make sure to encode the label vector\n",
    "encoded_label_vector = encode_label_vector(train_label)\n",
    "\n",
    "hidden_layers = 3 # this should be int\n",
    "hidden_layers_activation_func = ['relu', 'relu', 'relu']\n",
    "num_neurons = [5,3,6] # this should be a list containing int per hidden layer\n",
    "num_classes = np.unique(train_label).size\n",
    "\n",
    "weight_matrix = []\n",
    "bias_vector = []\n",
    "\n",
    "# initialise the weights\n",
    "for layer_num, layer in enumerate(range(hidden_layers)):\n",
    "    \n",
    "    # If we are instantiating the details for the first hidden layer, then make the following adjustments\n",
    "    # which would otherwise be not required for subsequent hidden layers\n",
    "    if layer_num == 0:\n",
    "        # The input features would be the shape of our dataset instead of num of features from previous layer\n",
    "        num_input_features = train_data.shape[1]\n",
    "    else:\n",
    "        num_input_features = num_neurons[layer_num - 1]\n",
    "    \n",
    "    # check how many neurons should be in this layer\n",
    "    neuron_num = num_neurons[layer_num]\n",
    "    \n",
    "    layer_weights = generate_gaussian_weights(neuron_num, num_input_features)\n",
    "    layer_bias = generate_gaussian_bias(neuron_num)\n",
    "    \n",
    "    weight_matrix.append(layer_weights)\n",
    "    bias_vector.append(layer_bias)\n",
    "    \n",
    "    print(f'Weight and bias generated for hidden layer {layer_num + 1} with weight shape {weight_matrix[layer_num].shape} and bias shape of {bias_vector[layer_num].shape}')\n",
    "\n",
    "   \n",
    "# insantiate the parts for the output layer\n",
    "# need to be very careful with the use of -1 indices, in the event that we incorporate output layer to our hidden layer variables\n",
    "weight_matrix.append(generate_gaussian_weights(num_classes, num_neurons[-1]))\n",
    "bias_vector.append(generate_gaussian_bias(num_classes))\n",
    "\n",
    "\n",
    "# feedforward part\n",
    "layer_z = []\n",
    "layer_output = []\n",
    "for layer_num, layer in enumerate(range(hidden_layers)):\n",
    "    \n",
    "    if layer_num == 0:\n",
    "        input_data = train_data[0] # hard coding this for now, will set batches later\n",
    "    else:\n",
    "        # extract the output of the previous layer\n",
    "        input_data = layer_output[layer_num - 1]\n",
    "        \n",
    "    z = calc_z(input_data, weight_matrix[layer_num], bias_vector[layer_num])\n",
    "    a = run_activation_func(hidden_layers_activation_func[layer_num], z)\n",
    "    \n",
    "    layer_output.append(a)\n",
    "    \n",
    "    \n",
    "# Calculation for the output layer\n",
    "z = calc_z(layer_output[-1], weight_matrix[-1], bias_vector[-1])\n",
    "a = run_activation_func('softmax', z)\n",
    "layer_z.append(z) # to be used for backpropagation\n",
    "layer_output.append(a)\n",
    "\n",
    "# Calculate the error\n",
    "print(calculate_MSE(layer_output[-1], encoded_label_vector[0]))\n",
    "\n",
    "\n",
    "# Perform the backpropagation\n",
    "# instantiate the delta list for hidden_layers + output layer\n",
    "delta = [0 for i in range(hidden_layers + 1)]\n",
    "\n",
    "# for the output layer\n",
    "delta[hidden_layers] = (calc_delta_softmax(layer_output[-1], encoded_label_vector[0])) # note that the zero is hardcoded for example 0\n",
    "\n",
    "for layer_num, layer in reversed(list(enumerate(range(hidden_layers)))):\n",
    "    # basically prepending to the start of the list, in order to retain layer order in delta list/matrix\n",
    "    print(f'Layer num is {layer_num}')\n",
    "    delta[layer_num] = calc_delta_hidden(delta[layer_num + 1], weight_matrix[layer_num + 1], layer_output[layer_num])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
