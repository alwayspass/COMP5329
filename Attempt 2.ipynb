{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "diverse-morocco",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import numpy as np\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cardiovascular-hurricane",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_data = np.load(\"./Assignment1-Dataset/train_data.npy\")\n",
    "train_label = np.load(\"./Assignment1-Dataset/train_label.npy\")\n",
    "test_data = np.load(\"./Assignment1-Dataset/test_data.npy\")\n",
    "test_label = np.load(\"./Assignment1-Dataset/test_label.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "patient-michael",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 128)\n",
      "(50000, 1)\n",
      "(10000, 128)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks\n",
    "print(train_data.shape)\n",
    "print(train_label.shape)\n",
    "print(test_data.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "brown-organization",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gaussian_weights(num_neurons, num_features):\n",
    "    '''\n",
    "    Generate weights taken from the Gaussian distribution, and based on the number of neurons within the hidden layer\n",
    "    as well as the number of features of the dataset\n",
    "    \n",
    "    Output is weights matrix of size (num_neurons, num_features)\n",
    "    '''\n",
    "    \n",
    "    weights = norm.rvs(size = [num_neurons, num_features], random_state=1)\n",
    "    \n",
    "    return weights\n",
    "\n",
    "def generate_gaussian_bias(num_neurons):\n",
    "    '''\n",
    "    Generate bias vector from the Gaussian distribution, and based on the number of neurons within the hidden layer\n",
    "    \n",
    "    Output is bias vector of size (num_neurons)\n",
    "    '''\n",
    "    \n",
    "    bias = norm.rvs(size = num_neurons, random_state=1)\n",
    "    \n",
    "    return bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "large-tactics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_z(data_vector, weights_matrix, bias_vector):\n",
    "    '''\n",
    "    Calculate the z value for all the neurons within the specific hidden layer, obtained by taking the dot product\n",
    "    between the weights matrix and data vector.  The bias vector is then added onto the product of the two.\n",
    "    \n",
    "    The output vector then represents the input value to be used for the activation function of all the neurons within\n",
    "    the specific hidden layer.\n",
    "    '''\n",
    "    \n",
    "    return weights_matrix.dot(data_vector) + bias_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "strong-information",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_activation_func(activation_func, z):\n",
    "    '''\n",
    "    Calculates the value after the z has been computed and puts it inside the non-linear activation function\n",
    "    that we have for that hidden layer\n",
    "    '''\n",
    "    \n",
    "    if activation_func == 'relu':\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    if activation_func == 'softmax':\n",
    "        print(z)\n",
    "        return np.divide(np.exp(z), np.sum(np.exp(z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "addressed-payroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_label_vector(label_vector):\n",
    "    '''\n",
    "    Encode the label vector of our dataset, such that we can use it for the computation of the MSE.\n",
    "    This is because the labels are labelled as integers 0 to 9, whereas for MSE to work, we need to\n",
    "    create a array vector of size 10 for every single observation, where the value 1 is set on the index of the correct observation\n",
    "    '''\n",
    "    \n",
    "    num_classes = np.unique(train_label).size\n",
    "    \n",
    "    encoded_label_vector = []\n",
    "    \n",
    "    for label in label_vector:\n",
    "        encoded_label = np.zeros(num_classes)\n",
    "        encoded_label[label] = 1\n",
    "        encoded_label_vector.append(encoded_label)\n",
    "    \n",
    "    encoded_label_vector = np.array(encoded_label_vector)\n",
    "    \n",
    "    return encoded_label_vector\n",
    "    \n",
    "\n",
    "def calculate_MSE(pred, actual):\n",
    "    '''\n",
    "    Calculates the Mean Squared Error between the prediction of the NN and actual class\n",
    "    \n",
    "    Note:\n",
    "    This works because the pred value is the softmax of the output of the NN and\n",
    "    the actual is adjusted such that the values are between 0 and 1\n",
    "    '''\n",
    "    error = np.subtract(pred, actual)\n",
    "    squared_error = np.square(error)\n",
    "    return np.sum(squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "registered-utility",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_delta_softmax(layer_output, encoded_label_vector):\n",
    "    '''\n",
    "    Calculates the delta value for the final/output layer, which will be used for backpropagation\n",
    "    \n",
    "    Note:\n",
    "    This function expects to receive the output of the softmax activation function within the output layer\n",
    "    and also the encoded label vector which would have values 0 and 1 exclusively\n",
    "    \n",
    "    delta = activation_output - y\n",
    "    \n",
    "    Output: vector of size num of classes to be predicted\n",
    "    '''\n",
    "    \n",
    "    return np.subtract(layer_output, encoded_label_vector)\n",
    "\n",
    "\n",
    "def calc_delta_hidden(l_plus_one_delta, l_plus_one_weights, layer_output):\n",
    "    '''\n",
    "    Calculates the delta value for the hidden layers, which will be used for backpropagation\n",
    "    \n",
    "    This function is only to be used to calculate the delta values of the hidden layers.  Output layer delta should be\n",
    "    the calc_delta_softmax, and there is no delta term needed to be calculated for input layer (NN layer 1)\n",
    "    \n",
    "    delta = (weights for next layer . delta for next layer) .* (activation_func output for this layer .* (1 - activation_func output for this layer))\n",
    "    '''\n",
    "    \n",
    "    first_part = (l_plus_one_weights.T).dot(l_plus_one_delta)\n",
    "    second_part = np.multiply(layer_output, np.ones(layer_output.size) - layer_output)\n",
    "    return  np.multiply(first_part, second_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "latter-times",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight and bias generated for hidden layer 1 with weight shape (5, 128) and bias shape of (5,)\n",
      "Weight and bias generated for hidden layer 2 with weight shape (3, 5) and bias shape of (3,)\n",
      "Weight and bias generated for hidden layer 3 with weight shape (6, 3) and bias shape of (6,)\n"
     ]
    }
   ],
   "source": [
    "# Neural Network Setup\n",
    "\n",
    "# Static variables\n",
    "HIDDEN_LAYERS: int = 3\n",
    "HIDDEN_LAYERS_ACTIVATION_FUNC: list = ['relu', 'relu', 'relu']\n",
    "NUM_NEURONS: list = [5,3,6] # this should be a list containing int per hidden layer\n",
    "    \n",
    "BATCH_SIZE: int = 3\n",
    "LEARNING_RATE: float = 0.001\n",
    "NUM_EPOCHS: int = 2\n",
    "\n",
    "    \n",
    "# Initialisation\n",
    "encoded_label_vector = encode_label_vector(train_label)\n",
    "num_classes = np.unique(train_label).size\n",
    "\n",
    "weight_matrix = []\n",
    "bias_vector = []\n",
    "epoch_loss = []\n",
    "\n",
    "# initialise the weights\n",
    "for layer_num, layer in enumerate(range(HIDDEN_LAYERS)):\n",
    "    \n",
    "    # If we are instantiating the details for the first hidden layer, then make the following adjustments\n",
    "    # which would otherwise be not required for subsequent hidden layers\n",
    "    if layer_num == 0:\n",
    "        # The input features would be the shape of our dataset instead of num of features from previous layer\n",
    "        num_input_features = train_data.shape[1]\n",
    "    else:\n",
    "        num_input_features = NUM_NEURONS[layer_num - 1]\n",
    "    \n",
    "    # check how many neurons should be in this layer\n",
    "    neuron_num = NUM_NEURONS[layer_num]\n",
    "    \n",
    "    layer_weights = generate_gaussian_weights(neuron_num, num_input_features)\n",
    "    layer_bias = generate_gaussian_bias(neuron_num)\n",
    "    \n",
    "    weight_matrix.append(layer_weights)\n",
    "    bias_vector.append(layer_bias)\n",
    "    \n",
    "    print(f'Weight and bias generated for hidden layer {layer_num + 1} with weight shape {weight_matrix[layer_num].shape} and bias shape of {bias_vector[layer_num].shape}')\n",
    "\n",
    "   \n",
    "# insantiate the parts for the output layer\n",
    "# need to be very careful with the use of -1 indices, in the event that we incorporate output layer to our hidden layer variables\n",
    "weight_matrix.append(generate_gaussian_weights(num_classes, NUM_NEURONS[-1]))\n",
    "bias_vector.append(generate_gaussian_bias(num_classes))\n",
    "\n",
    "weight_matrix = np.array(weight_matrix, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "printable-husband",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-67.35735725 -34.26694632 -64.32131713  70.53327837 -63.57160494\n",
      " -54.21631103  44.08017846  16.80783561   0.96140717  61.19887047]\n",
      "[ 5.01177359  3.48773985 -1.20110933 -0.22415543  2.09687643 -4.00593615\n",
      " -0.23610207 -1.80963029 -0.47452523 -0.3415373 ]\n",
      "[ 43.80477955 118.00669812  53.71973838 -34.21490893   7.44403297\n",
      " -86.78948415  66.32050895 -10.46533466 -19.73615087  55.68753047]\n",
      "[1.8730641500029304]\n",
      "[ 4.89628014e+12  6.26242439e+12 -9.73397078e+11  1.84911576e+12\n",
      "  1.25133783e+12 -2.67839198e+12 -2.57860331e+12 -1.09914326e+12\n",
      " -2.00088912e+12  1.90213693e+11]\n",
      "[ 1.88372928e+12  2.40932134e+12 -3.74491763e+11  7.11404048e+11\n",
      "  4.81422966e+11 -1.03044868e+12 -9.92057321e+11 -4.22869665e+11\n",
      " -7.69795296e+11  7.31802703e+10]\n",
      "[ 6.58023603e+12  8.41623222e+12 -1.30817322e+12  2.48507394e+12\n",
      "  1.68170489e+12 -3.59955945e+12 -3.46545090e+12 -1.47716672e+12\n",
      " -2.68904603e+12  2.55633045e+11]\n",
      "[1.8730641500029304, nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-2cd2b72e1094>:12: RuntimeWarning: overflow encountered in exp\n",
      "  return np.divide(np.exp(z), np.sum(np.exp(z)))\n",
      "<ipython-input-27-2cd2b72e1094>:12: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.divide(np.exp(z), np.sum(np.exp(z)))\n"
     ]
    }
   ],
   "source": [
    "### Getting the network running\n",
    "for epoch_num in range(NUM_EPOCHS):\n",
    "    batch_loss = []\n",
    "\n",
    "    # feedforward part\n",
    "    # for the two variables below, the expected final state is numpy_array(representing each layer)\n",
    "    # in a list (representing each observation)\n",
    "    # in a list (the final container of the object)\n",
    "    layer_z = [[] for i in range(BATCH_SIZE)]\n",
    "    layer_output = [[] for i in range(BATCH_SIZE)]\n",
    "\n",
    "    for observation_idx, observation_val in enumerate(range(BATCH_SIZE)):\n",
    "\n",
    "        for layer_num, layer in enumerate(range(HIDDEN_LAYERS)):\n",
    "\n",
    "            if layer_num == 0:\n",
    "                input_data = train_data[observation_idx]\n",
    "            else:\n",
    "                # extract the output of the previous layer\n",
    "                input_data = layer_output[observation_idx][layer_num - 1]\n",
    "\n",
    "            z = calc_z(input_data, weight_matrix[layer_num], bias_vector[layer_num])\n",
    "            a = run_activation_func(HIDDEN_LAYERS_ACTIVATION_FUNC[layer_num], z)\n",
    "\n",
    "            layer_z[observation_idx].append(z)\n",
    "            layer_output[observation_idx].append(a)\n",
    "\n",
    "        # Calculation for the output layer\n",
    "        z = calc_z(layer_output[observation_idx][-1], weight_matrix[-1], bias_vector[-1])\n",
    "        a = run_activation_func('softmax', z)\n",
    "        layer_z[observation_idx].append(z) # to be used for backpropagation\n",
    "        layer_output[observation_idx].append(a)\n",
    "\n",
    "        # Calculate the error\n",
    "        loss = calculate_MSE(layer_output[observation_idx][-1], encoded_label_vector[observation_idx]) \n",
    "        batch_loss.append(loss)\n",
    "\n",
    "    epoch_loss.append(np.average(batch_loss))\n",
    "    print(epoch_loss)\n",
    "\n",
    "    # Perform the backpropagation\n",
    "    # instantiate the delta list obj for HIDDEN_LAYERS + output layer\n",
    "    delta = [[0 for i in range(HIDDEN_LAYERS + 1)] for i in range(BATCH_SIZE)]\n",
    "    d = [[0 for i in range(HIDDEN_LAYERS + 1)] for i in range(BATCH_SIZE)]\n",
    "\n",
    "    for observation_idx, observation_val in enumerate(range(BATCH_SIZE)):\n",
    "        # for the output layer\n",
    "        delta[observation_idx][HIDDEN_LAYERS] = (calc_delta_softmax(layer_output[observation_idx][-1], encoded_label_vector[observation_idx]))\n",
    "        d[observation_idx][HIDDEN_LAYERS] = np.outer(delta[observation_idx][HIDDEN_LAYERS].T, layer_output[observation_idx][HIDDEN_LAYERS - 1])\n",
    "\n",
    "        for layer_num, layer in reversed(list(enumerate(range(HIDDEN_LAYERS)))):\n",
    "\n",
    "            delta[observation_idx][layer_num] = calc_delta_hidden(delta[observation_idx][layer_num + 1], weight_matrix[layer_num + 1], layer_output[observation_idx][layer_num])\n",
    "\n",
    "            if layer_num == 0:\n",
    "                # Because we're at layer 0 (the first hidden layer), then we need to use the raw data inputs as the layer_output of the input layer\n",
    "                # note that the input_layer is not included in our layer_output list\n",
    "                d[observation_idx][layer_num] = np.outer(delta[observation_idx][layer_num], train_data[observation_idx].T)\n",
    "            else:\n",
    "                d[observation_idx][layer_num] = np.outer(delta[observation_idx][layer_num], layer_output[observation_idx][layer_num - 1].T)\n",
    "\n",
    "\n",
    "    # Time to calculate the average delta from across different observations\n",
    "    d = np.array(d, dtype=object) # quite important to convert this list into a pure numpy array for avg next\n",
    "    avg_d = np.average(d, axis = 0) # each array within the array, then represents the neurons within a layer\n",
    "\n",
    "\n",
    "    # Time to update our weights, using the averaged delta from previous calc\n",
    "    update = np.array(LEARNING_RATE * avg_d)\n",
    "    weight_matrix = np.subtract(weight_matrix, update)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
